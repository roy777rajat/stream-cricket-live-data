{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Session Configuration with Delta Lake and S3 Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CricketDeltaPipeline\") \\\n",
    "    .config(\"spark.jars\", \",\".join([\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\delta-core_2.12-3.1.0.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\delta-storage-3.1.0.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\hadoop-aws-3.3.4.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\aws-java-sdk-bundle-1.12.430.jar\"\n",
    "    ])) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Raw Parquet from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|                  id|                name|matchType|          event_time|         ingested_at|           json_data|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|f963354a-5c28-46b...|Malawi vs Germany...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|e9f89b3e-3c0e-41e...|Italy vs Guernsey...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|25c9a554-e47c-47d...|Netherlands vs Je...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|f4556647-3fff-451...|Australia A vs Sr...|      odi|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|25628cc1-c8ec-4e5...|Singapore Women v...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|222117a5-804b-419...|England U19 vs In...|      odi|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|e80592f5-2dc3-40f...|West Indies vs Au...|     test|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|efb7ef80-ac47-423...|Sri Lanka vs Bang...|      odi|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|52d2a3d5-ae09-46a...|Washington Freedo...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|e8b6213e-ccd6-45b...|San Francisco Uni...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|f14e623b-78a8-41f...|Los Angeles Knigh...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|b4462d46-7c7b-4b0...|Chepauk Super Gil...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|0034265a-69c6-4b8...|Lancashire vs Der...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|e8e2b435-eda5-40e...|Durham vs Notting...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|fde4bfef-4962-470...|Essex vs Gloucest...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|936b2031-898d-423...|Kent vs Sussex, S...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|0410ad34-ddd0-4d3...|Leicestershire vs...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|df4b066d-8da8-48c...|Northamptonshire ...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|19602a0a-a18e-42f...|Somerset vs Glamo...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "|800a9b28-b65f-4de...|Yorkshire vs Worc...|      t20|2025-07-05 17:41:...|2025-07-05 15:57:...|{\"event_time\": \"2...|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "now = datetime.utcnow().date()\n",
    "s3_path = f\"s3a://aws-glue-assets-cricket/raw_cricket_data/year={now.year}/month={now.month:02}/day={now.day:02}/*/\"\n",
    "df = spark.read.parquet(s3_path)\n",
    "df = df.withColumn(\"event_time_ts\", F.to_timestamp(\"event_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deduplicate Rows by ID + Latest Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(F.col(\"event_time_ts\").desc())\n",
    "df = df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "latest_df = df.filter(F.col(\"row_num\") == 1).drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Match Status Based on JSON Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "json_schema_status = StructType([\n",
    "    StructField(\"matchStarted\", StringType(), True),\n",
    "    StructField(\"matchEnded\", StringType(), True),\n",
    "    StructField(\"score\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "latest_df = latest_df.withColumn(\"json_data_parsed\", F.from_json(F.col(\"json_data\"), json_schema_status))\n",
    "latest_df = latest_df.withColumn(\"match_status\", F.when(\n",
    "    (F.col(\"json_data_parsed.matchStarted\") == \"true\") &\n",
    "    (F.col(\"json_data_parsed.matchEnded\") == \"false\") &\n",
    "    (F.size(F.col(\"json_data_parsed.score\")) > 0), \"Live\")\n",
    "    .when((F.col(\"json_data_parsed.matchStarted\") == \"true\") &\n",
    "          (F.col(\"json_data_parsed.matchEnded\") == \"false\") &\n",
    "          (F.size(F.col(\"json_data_parsed.score\")) == 0), \"Upcoming\")\n",
    "    .when((F.col(\"json_data_parsed.matchEnded\") == \"true\"), \"Completed\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#latest_df.show(truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generic Method to extract Matches (be it Live or Non-Live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_status_wise_filteration (filterText):\n",
    "    json_schema_teams = StructType([\n",
    "        StructField(\"venue\", StringType(), True),\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"teamInfo\", ArrayType(StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"shortname\", StringType(), True),\n",
    "            StructField(\"img\", StringType(), True)\n",
    "        ])), True)\n",
    "    ])\n",
    "    if filterText == \"Live\":\n",
    "        live_df = latest_df.filter(F.col(\"match_status\") == filterText) \\\n",
    "            .withColumn(\"json_data_parsed\", F.from_json(F.col(\"json_data\"), json_schema_teams)) \\\n",
    "            .select(\n",
    "                \"id\", \"name\", F.upper(\"matchType\").alias(\"matchType\"),\n",
    "                F.col(\"json_data_parsed.venue\"),\n",
    "                F.col(\"json_data_parsed.date\"),\n",
    "                \"match_status\",\n",
    "                F.col(\"json_data_parsed.teamInfo\")[0][\"name\"].alias(\"team1_name\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[0][\"shortname\"].alias(\"team1_shortname\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[0][\"img\"].alias(\"team1_img\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[1][\"name\"].alias(\"team2_name\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[1][\"shortname\"].alias(\"team2_shortname\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[1][\"img\"].alias(\"team2_img\"),\n",
    "                F.date_format(F.current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").alias(\"EffectiveDateTime\")\n",
    "            )\n",
    "        return live_df\n",
    "    else:\n",
    "        non_live_df = latest_df.filter(F.col(\"match_status\") != \"Live\") \\\n",
    "            .withColumn(\"json_data_parsed\", F.from_json(F.col(\"json_data\"), json_schema_teams)) \\\n",
    "            .select(\n",
    "                \"id\", \"name\", F.upper(\"matchType\").alias(\"matchType\"),\n",
    "                F.col(\"json_data_parsed.venue\"),\n",
    "                F.col(\"json_data_parsed.date\"),\n",
    "                \"match_status\",\n",
    "                F.col(\"json_data_parsed.teamInfo\")[0][\"name\"].alias(\"team1_name\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[0][\"shortname\"].alias(\"team1_shortname\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[0][\"img\"].alias(\"team1_img\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[1][\"name\"].alias(\"team2_name\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[1][\"shortname\"].alias(\"team2_shortname\"),\n",
    "                F.col(\"json_data_parsed.teamInfo\")[1][\"img\"].alias(\"team2_img\"),\n",
    "                F.date_format(F.current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").alias(\"EffectiveDateTime\")\n",
    "            )\n",
    "        return non_live_df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb3dd2",
   "metadata": {},
   "source": [
    "## Fetch LIVE Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c51eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------------------------------------+---------+---------------------------------------------+----------+------------+-----------+---------------+----------------------------------------------------------+-----------+---------------+-----------------------------------------------------------+-------------------+\n",
      "|id                                  |name                                   |matchType|venue                                        |date      |match_status|team1_name |team1_shortname|team1_img                                                 |team2_name |team2_shortname|team2_img                                                  |EffectiveDateTime  |\n",
      "+------------------------------------+---------------------------------------+---------+---------------------------------------------+----------+------------+-----------+---------------+----------------------------------------------------------+-----------+---------------+-----------------------------------------------------------+-------------------+\n",
      "|222117a5-804b-4194-be23-0c42b879b1ed|England U19 vs India U19, 4th Youth ODI|ODI      |New Road, Worcester                          |2025-07-05|Live        |England U19|ENG            |https://g.cricapi.com/iapi/23-637877072894080569.webp?w=48|India U19  |IN19           |https://h.cricapi.com/img/icon512.png                      |2025-07-05 18:28:23|\n",
      "|e80592f5-2dc3-40fc-80f9-cc58d193bae8|West Indies vs Australia, 2nd Test     |TEST     |National Cricket Stadium, St Georges, Grenada|2025-07-03|Live        |Australia  |AUS            |https://g.cricapi.com/iapi/6-637877070670541994.webp?w=48 |West Indies|WI             |https://g.cricapi.com/iapi/100-637877077978319234.webp?w=48|2025-07-05 18:28:23|\n",
      "|f339af88-470d-4572-b5b0-9008d4f0d22a|England vs India, 2nd Test             |TEST     |Edgbaston, Birmingham                        |2025-07-02|Live        |England    |ENG            |https://g.cricapi.com/iapi/23-637877072894080569.webp?w=48|India      |IND            |https://g.cricapi.com/iapi/31-637877061080567215.webp?w=48 |2025-07-05 18:28:23|\n",
      "+------------------------------------+---------------------------------------+---------+---------------------------------------------+----------+------------+-----------+---------------+----------------------------------------------------------+-----------+---------------+-----------------------------------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "live_df = match_status_wise_filteration(\"Live\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08124ea",
   "metadata": {},
   "source": [
    "## Fetch Non-Live Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4b87fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_live_df = match_status_wise_filteration(\"NonLive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9899bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#non_live_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Delta Upsert Function (foreachBatch) - LIVE Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, col\n",
    "\n",
    "# Define S3 paths\n",
    "target_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/cricket_data\"\n",
    "checkpoint_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/cricket_data/checkpoints\"\n",
    "\n",
    "# Add partition columns to live_df\n",
    "live_df = live_df.withColumn(\"year\", year(col(\"EffectiveDateTime\"))) \\\n",
    "                 .withColumn(\"month\", month(col(\"EffectiveDateTime\"))) \\\n",
    "                 .withColumn(\"day\", dayofmonth(col(\"EffectiveDateTime\")))\n",
    "\n",
    "# Define upsert function using Delta SQL\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "\n",
    "    # Check if Delta table exists\n",
    "    try:\n",
    "        spark.read.format(\"delta\").load(target_path)\n",
    "        table_exists = True\n",
    "    except Exception as e:\n",
    "        table_exists = False\n",
    "\n",
    "    if not table_exists:\n",
    "        # Create the Delta table if it does not exist\n",
    "        microBatchDF.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(target_path)\n",
    "    else:\n",
    "        # Perform upsert (merge) if table exists\n",
    "        microBatchDF.createOrReplaceTempView(\"source_table\")\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO delta.`{target_path}` AS target\n",
    "            USING source_table AS source\n",
    "            ON target.id = source.id AND target.date = source.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "# Call for batch upsert\n",
    "upsert_to_delta(live_df, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b773ae9",
   "metadata": {},
   "source": [
    "## 6. Define Delta Upsert Function (foreachBatch) - NON LIVE Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "24fdb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, col\n",
    "\n",
    "# Define S3 paths\n",
    "target_path = \"s3a://aws-glue-assets-cricket/output_cricket/nonlive/cricket_data\"\n",
    "checkpoint_path = \"s3a://aws-glue-assets-cricket/output_cricket/nonlive/cricket_data/checkpoints\"\n",
    "\n",
    "# Add partition columns to live_df\n",
    "non_live_df = non_live_df.withColumn(\"year\", year(col(\"EffectiveDateTime\"))) \\\n",
    "                 .withColumn(\"month\", month(col(\"EffectiveDateTime\"))) \\\n",
    "                 .withColumn(\"day\", dayofmonth(col(\"EffectiveDateTime\")))\n",
    "\n",
    "# Define upsert function using Delta SQL\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "\n",
    "    # Check if Delta table exists\n",
    "    try:\n",
    "        spark.read.format(\"delta\").load(target_path)\n",
    "        table_exists = True\n",
    "    except Exception as e:\n",
    "        table_exists = False\n",
    "\n",
    "    if not table_exists:\n",
    "        # Create the Delta table if it does not exist\n",
    "        microBatchDF.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(target_path)\n",
    "    else:\n",
    "        # Perform upsert (merge) if table exists\n",
    "        microBatchDF.createOrReplaceTempView(\"source_table\")\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO delta.`{target_path}` AS target\n",
    "            USING source_table AS source\n",
    "            ON target.id = source.id AND target.date = source.date\n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "# Call for batch upsert\n",
    "upsert_to_delta(non_live_df, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
