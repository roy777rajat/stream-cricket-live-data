{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92bc166",
   "metadata": {},
   "source": [
    "## 0. Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7d6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrickeScoree\") \\\n",
    "    .config(\"spark.jars\", \",\".join([\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\delta-core_2.12-3.1.0.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\delta-storage-3.1.0.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\hadoop-aws-3.3.4.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\aws-java-sdk-bundle-1.12.430.jar\"\n",
    "    ])) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47805700",
   "metadata": {},
   "source": [
    "## Process the data in micro-batch way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c647a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, when, size, explode, max as Fmax, year, month, dayofmonth\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, ArrayType, IntegerType, FloatType\n",
    "from datetime import datetime\n",
    "\n",
    "# JSON schema for stream data\n",
    "json_schema = StructType([\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"venue\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"dateTimeGMT\", StringType()),\n",
    "    StructField(\"teams\", ArrayType(StringType())),\n",
    "    StructField(\"teamInfo\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"img\", StringType())\n",
    "        ])\n",
    "    )),\n",
    "    StructField(\"score\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"r\", IntegerType()),\n",
    "            StructField(\"w\", IntegerType()),\n",
    "            StructField(\"o\", FloatType()),\n",
    "            StructField(\"inning\", StringType())\n",
    "        ])\n",
    "    )),\n",
    "    StructField(\"series_id\", StringType()),\n",
    "    StructField(\"fantasyEnabled\", BooleanType()),\n",
    "    StructField(\"bbbEnabled\", BooleanType()),\n",
    "    StructField(\"hasSquad\", BooleanType()),\n",
    "    StructField(\"matchStarted\", BooleanType()),\n",
    "    StructField(\"matchEnded\", BooleanType())\n",
    "])\n",
    "\n",
    "# Paths\n",
    "target_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/score_data\"\n",
    "checkpoint_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints\"\n",
    "base_static_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/cricket_data\"\n",
    "\n",
    "# Load static metadata for today's partition\n",
    "def load_static_match_data(spark):\n",
    "    today = datetime.utcnow().date()\n",
    "    path = f\"{base_static_path}/year={today.year}/month={today.month}/day={today.day}\"\n",
    "    static_df = spark.read.option(\"basePath\", base_static_path).parquet(path)\n",
    "    static_df = static_df.dropDuplicates([\"id\"])\n",
    "    return static_df\n",
    "\n",
    "# Process streaming micro-batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    static_df = load_static_match_data(batch_df.sparkSession)\n",
    "\n",
    "    conflicting_cols = [\"matchType\", \"name\", \"match_status\", \"venue\"]\n",
    "    for c in conflicting_cols:\n",
    "        if c in static_df.columns:\n",
    "            static_df = static_df.drop(c)\n",
    "\n",
    "    static_df = static_df.withColumnRenamed(\"id\", \"match_id\")\n",
    "\n",
    "    parsed_df = batch_df.withColumn(\"json_parsed\", from_json(col(\"json_data\"), json_schema))\n",
    "\n",
    "    flat_df = parsed_df.select(\n",
    "        \"id\", \"name\", \"matchType\", \"event_time\",\n",
    "        col(\"json_parsed.status\").alias(\"status\"),\n",
    "        col(\"json_parsed.venue\").alias(\"venue\"),\n",
    "        col(\"json_parsed.teams\").alias(\"teams\"),\n",
    "        col(\"json_parsed.score\").alias(\"score\"),\n",
    "        col(\"json_parsed.matchStarted\").alias(\"matchStarted\"),\n",
    "        col(\"json_parsed.matchEnded\").alias(\"matchEnded\")\n",
    "    ).withColumn(\"event_time_ts\", col(\"event_time\").cast(\"timestamp\"))\n",
    "\n",
    "    max_times = flat_df.groupBy(\"id\").agg(Fmax(\"event_time_ts\").alias(\"max_ts\")) \\\n",
    "                       .withColumnRenamed(\"id\", \"max_id\")\n",
    "\n",
    "    latest_df = flat_df.join(\n",
    "        max_times,\n",
    "        (flat_df.id == max_times.max_id) & (flat_df.event_time_ts == max_times.max_ts),\n",
    "        \"inner\"\n",
    "    ).drop(\"max_id\", \"max_ts\")\n",
    "\n",
    "    latest_df = latest_df.withColumn(\n",
    "        \"match_status\",\n",
    "        when((col(\"matchStarted\") == True) & (col(\"matchEnded\") == False) & (size(col(\"score\")) > 0), \"Live\")\n",
    "        .when((col(\"matchStarted\") == True) & (col(\"matchEnded\") == False), \"Upcoming\")\n",
    "        .when(col(\"matchEnded\") == True, \"Completed\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    )\n",
    "\n",
    "    live_df = latest_df.filter(col(\"match_status\") == \"Live\")\n",
    "\n",
    "    exploded_df = live_df.select(\n",
    "        \"id\", \"name\", \"matchType\", \"event_time_ts\", \"status\", \"venue\", \"teams\", \"match_status\",\n",
    "        explode(col(\"score\")).alias(\"score_entry\")\n",
    "    )\n",
    "\n",
    "    final_df = exploded_df.select(\n",
    "        col(\"id\").alias(\"match_id\"),\n",
    "        \"name\",\n",
    "        \"matchType\",\n",
    "        \"event_time_ts\",\n",
    "        \"status\",\n",
    "        \"venue\",\n",
    "        \"teams\",\n",
    "        \"match_status\",\n",
    "        col(\"score_entry.inning\").alias(\"inning\"),\n",
    "        col(\"score_entry.r\").alias(\"runs\"),\n",
    "        col(\"score_entry.w\").alias(\"wickets\"),\n",
    "        col(\"score_entry.o\").alias(\"overs\")\n",
    "    ).dropDuplicates([\"match_id\", \"inning\", \"event_time_ts\"])\n",
    "\n",
    "    enriched_df = final_df.join(static_df, on=\"match_id\", how=\"left\")\n",
    "\n",
    "    # ðŸ†• Add partition columns before writing\n",
    "    enriched_df = enriched_df.withColumn(\"year\", year(\"event_time_ts\")) \\\n",
    "                             .withColumn(\"month\", month(\"event_time_ts\")) \\\n",
    "                             .withColumn(\"day\", dayofmonth(\"event_time_ts\"))\n",
    "\n",
    "    enriched_df.write \\\n",
    "        .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .parquet(target_path)\n",
    "\n",
    "# Example streaming job start (you already know how to plug in your Kafka stream here)\n",
    "# query = streaming_df.writeStream.foreachBatch(process_batch).option(\"checkpointLocation\", checkpoint_path).start()\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3750db",
   "metadata": {},
   "source": [
    "## This is juts to test purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb792a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o37.load.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/sources/0\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/commits\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/offsets\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\r\n\tat scala.Predef$.assert(Predef.scala:223)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:110)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:201)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:75)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:51)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m s3_path_score_live \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://aws-glue-assets-cricket/output_cricket/live/score_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Read parquet batch (not streaming)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_path_score_live\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 3: Print schema and show some sample rows\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o37.load.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/sources/0\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/commits\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data\n\ts3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/offsets\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\r\n\tat scala.Predef$.assert(Predef.scala:223)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:110)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:201)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:75)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:51)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "#  from datetime import datetime\n",
    "\n",
    "# # Step 1: Get UTC time and build S3 input path\n",
    "# now = datetime.utcnow()\n",
    "# s3_path = f\"s3a://aws-glue-assets-cricket/raw_cricket_data/year={now.year}/month={now.strftime('%m')}/*/*/\"\n",
    "# s3_path_score_live =f\"s3a://aws-glue-assets-cricket/output_cricket/live/score_data/\"  \n",
    "# # Step 2: Read parquet batch (not streaming)\n",
    "# df = spark.read.format(\"parquet\").load(s3_path_score_live)\n",
    "\n",
    "# # Step 3: Print schema and show some sample rows\n",
    "# df.printSchema()\n",
    "# #Full output\n",
    "# # df.show(5, truncate=False)\n",
    "# df.show(30, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25025f",
   "metadata": {},
   "source": [
    "## Start the Stream with calling process_batch (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff205c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Streaming started...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Step 1: Get UTC time and build S3 input path\n",
    "now = datetime.utcnow()\n",
    "s3_path = f\"s3a://aws-glue-assets-cricket/raw_cricket_data/year={now.year}/month={now.strftime('%m')}/*/*/\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"matchType\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),  # Changed to TimestampType\n",
    "    StructField(\"ingested_at\", StringType(), True),    # Added missing field\n",
    "    StructField(\"json_data\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Step 2: Read stream from S3 (define 'schema' for raw data with 'json_data' and 'event_time')\n",
    "df = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(s3_path)\n",
    "\n",
    "# Step 3: Start structured streaming job\n",
    "query = df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Structured Streaming started...\")\n",
    "\n",
    "# Step 4: Keep it running\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
