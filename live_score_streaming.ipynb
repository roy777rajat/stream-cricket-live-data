{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92bc166",
   "metadata": {},
   "source": [
    "## 0. Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7d6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrickeScoree\") \\\n",
    "    .config(\"spark.jars\", \",\".join([\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\delta-core_2.12-3.1.0.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\delta-storage-3.1.0.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\hadoop-aws-3.3.4.jar\",\n",
    "        r\"C:\\\\spark\\\\spark-3.5.5-bin-hadoop3\\\\jars\\\\aws-java-sdk-bundle-1.12.430.jar\"\n",
    "    ])) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47805700",
   "metadata": {},
   "source": [
    "## Process the data in micro-batch way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, when, size, explode, max as Fmax, year, month, dayofmonth\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, ArrayType, IntegerType, FloatType\n",
    "from datetime import datetime\n",
    "\n",
    "# JSON schema for stream data\n",
    "json_schema = StructType([\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"venue\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"dateTimeGMT\", StringType()),\n",
    "    StructField(\"teams\", ArrayType(StringType())),\n",
    "    StructField(\"teamInfo\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"img\", StringType())\n",
    "        ])\n",
    "    )),\n",
    "    StructField(\"score\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"r\", IntegerType()),\n",
    "            StructField(\"w\", IntegerType()),\n",
    "            StructField(\"o\", FloatType()),\n",
    "            StructField(\"inning\", StringType())\n",
    "        ])\n",
    "    )),\n",
    "    StructField(\"series_id\", StringType()),\n",
    "    StructField(\"fantasyEnabled\", BooleanType()),\n",
    "    StructField(\"bbbEnabled\", BooleanType()),\n",
    "    StructField(\"hasSquad\", BooleanType()),\n",
    "    StructField(\"matchStarted\", BooleanType()),\n",
    "    StructField(\"matchEnded\", BooleanType())\n",
    "])\n",
    "\n",
    "# Paths\n",
    "target_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/score_data\"\n",
    "checkpoint_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints\"\n",
    "base_static_path = \"s3a://aws-glue-assets-cricket/output_cricket/live/cricket_data\"\n",
    "\n",
    "# Load static metadata for today's partition\n",
    "def load_static_match_data(spark):\n",
    "    today = datetime.utcnow().date()\n",
    "    path = f\"{base_static_path}/year={today.year}/month={today.month}/day={today.day}\"\n",
    "    static_df = spark.read.option(\"basePath\", base_static_path).parquet(path)\n",
    "    static_df = static_df.dropDuplicates([\"id\"])\n",
    "    return static_df\n",
    "\n",
    "# Process streaming micro-batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    static_df = load_static_match_data(batch_df.sparkSession)\n",
    "\n",
    "    conflicting_cols = [\"matchType\", \"name\", \"match_status\", \"venue\"]\n",
    "    for c in conflicting_cols:\n",
    "        if c in static_df.columns:\n",
    "            static_df = static_df.drop(c)\n",
    "\n",
    "    static_df = static_df.withColumnRenamed(\"id\", \"match_id\")\n",
    "\n",
    "    parsed_df = batch_df.withColumn(\"json_parsed\", from_json(col(\"json_data\"), json_schema))\n",
    "\n",
    "    flat_df = parsed_df.select(\n",
    "        \"id\", \"name\", \"matchType\", \"event_time\",\n",
    "        col(\"json_parsed.status\").alias(\"status\"),\n",
    "        col(\"json_parsed.venue\").alias(\"venue\"),\n",
    "        col(\"json_parsed.teams\").alias(\"teams\"),\n",
    "        col(\"json_parsed.score\").alias(\"score\"),\n",
    "        col(\"json_parsed.matchStarted\").alias(\"matchStarted\"),\n",
    "        col(\"json_parsed.matchEnded\").alias(\"matchEnded\")\n",
    "    ).withColumn(\"event_time_ts\", col(\"event_time\").cast(\"timestamp\"))\n",
    "\n",
    "    max_times = flat_df.groupBy(\"id\").agg(Fmax(\"event_time_ts\").alias(\"max_ts\")) \\\n",
    "                       .withColumnRenamed(\"id\", \"max_id\")\n",
    "\n",
    "    latest_df = flat_df.join(\n",
    "        max_times,\n",
    "        (flat_df.id == max_times.max_id) & (flat_df.event_time_ts == max_times.max_ts),\n",
    "        \"inner\"\n",
    "    ).drop(\"max_id\", \"max_ts\")\n",
    "\n",
    "    latest_df = latest_df.withColumn(\n",
    "        \"match_status\",\n",
    "        when((col(\"matchStarted\") == True) & (col(\"matchEnded\") == False) & (size(col(\"score\")) > 0), \"Live\")\n",
    "        .when((col(\"matchStarted\") == True) & (col(\"matchEnded\") == False), \"Upcoming\")\n",
    "        .when(col(\"matchEnded\") == True, \"Completed\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    )\n",
    "\n",
    "    live_df = latest_df.filter(col(\"match_status\") == \"Live\")\n",
    "\n",
    "    exploded_df = live_df.select(\n",
    "        \"id\", \"name\", \"matchType\", \"event_time_ts\", \"status\", \"venue\", \"teams\", \"match_status\",\n",
    "        explode(col(\"score\")).alias(\"score_entry\")\n",
    "    )\n",
    "\n",
    "    final_df = exploded_df.select(\n",
    "        col(\"id\").alias(\"match_id\"),\n",
    "        \"name\",\n",
    "        \"matchType\",\n",
    "        \"event_time_ts\",\n",
    "        \"status\",\n",
    "        \"venue\",\n",
    "        \"teams\",\n",
    "        \"match_status\",\n",
    "        col(\"score_entry.inning\").alias(\"inning\"),\n",
    "        col(\"score_entry.r\").alias(\"runs\"),\n",
    "        col(\"score_entry.w\").alias(\"wickets\"),\n",
    "        col(\"score_entry.o\").alias(\"overs\")\n",
    "    ).dropDuplicates([\"match_id\", \"event_time_ts\"])\n",
    "\n",
    "    enriched_df = final_df.join(static_df, on=\"match_id\", how=\"left\")\n",
    "\n",
    "    # ğŸ†• Add partition columns before writing\n",
    "    enriched_df = enriched_df.withColumn(\"year\", year(\"event_time_ts\")) \\\n",
    "                             .withColumn(\"month\", month(\"event_time_ts\")) \\\n",
    "                             .withColumn(\"day\", dayofmonth(\"event_time_ts\"))\n",
    "\n",
    "    enriched_df.write \\\n",
    "        .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .parquet(target_path)\n",
    "\n",
    "# Example streaming job start (you already know how to plug in your Kafka stream here)\n",
    "# query = streaming_df.writeStream.foreachBatch(process_batch).option(\"checkpointLocation\", checkpoint_path).start()\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25025f",
   "metadata": {},
   "source": [
    "## Start the Stream with calling process_batch (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff205c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Streaming started...\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 1c6d55b4-cdc7-4365-bb6d-678672558cb5, runId = 38f9c03c-aab1-41da-b4cf-bf3822017732] terminated with exception: No such file or directory: s3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/commits",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStructured Streaming started...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Step 4: Keep it running\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\streaming\\query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = 1c6d55b4-cdc7-4365-bb6d-678672558cb5, runId = 38f9c03c-aab1-41da-b4cf-bf3822017732] terminated with exception: No such file or directory: s3a://aws-glue-assets-cricket/output_cricket/live/score_data/checkpoints/commits"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Step 1: Get UTC time and build S3 input path\n",
    "now = datetime.utcnow()\n",
    "s3_path = f\"s3a://aws-glue-assets-cricket/raw_cricket_data/year={now.year}/month={now.month:02}/day={now.day:02}/*/*/\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"matchType\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),  # Changed to TimestampType\n",
    "    StructField(\"ingested_at\", StringType(), True),    # Added missing field\n",
    "    StructField(\"json_data\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Step 2: Read stream from S3 (define 'schema' for raw data with 'json_data' and 'event_time')\n",
    "df = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(s3_path)\n",
    "\n",
    "# Step 3: Start structured streaming job\n",
    "query = df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Structured Streaming started...\")\n",
    "\n",
    "# Step 4: Keep it running\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
